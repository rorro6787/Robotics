{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Simultaneous Localization and Mapping\n",
    "\n",
    "Simultaneous Localization And Mapping (SLAM) is one of the most fundamental problems in robotics.\n",
    "It arises when our robot doesn't have access to a map of the environment nor the pose where it is located.\n",
    "In this way, the SLAM problem is more complex than the two separate ones in the previous chapters, localization and mapping.\n",
    "\n",
    "There are mainly two ways to address the SLAM problem: **Full SLAM** and **Online SLAM**.\n",
    "\n",
    "## 7.1 Full SLAM\n",
    "\n",
    "Estimates the whole path traversed at each step, that is: \n",
    "\n",
    "$$p( x_{1:k}, m_{1:L} | z_{1:k}, u_{1:k})$$ \n",
    "\n",
    "This issue can be faced by means of the GraphSLAM technique, where landmarks and robot poses are represented as nodes in a graph linked by arcs stating odometry and/or observations. Its general idea is that arcs are constraints for the free movement of the nodes. $\\\\[10pt]$\n",
    "\n",
    "<center>\n",
    "    <img src=\"./images/graphslam.png\"/>\n",
    "    <figcaption>Fig 1. Example of SLAM with GraphSLAM.</figcaption>\n",
    "</center>\n",
    "\n",
    "A simplified version of this technique, Pose GraphSLAM, is typically prefered. In this case, nodes are unknown robot poses, and arcs represent odometry information or common observed landmarks from different poses. $\\\\[10pt]$\n",
    "\n",
    "<center>\n",
    "    <img src=\"./images/pose_graphslam.png\"/>\n",
    "    <figcaption>Fig 2. Example of SLAM with Pose GraphSLAM.</figcaption>\n",
    "</center>\n",
    "\n",
    "\n",
    "## 7.2 Online SLAM\n",
    "\n",
    "Which only estimates the latest pose (we do not consider $x_{1:k-1}$), so:\n",
    "\n",
    "$$p(x_{k}, m_{1:L} | z_{1:k}, u_{1:k})$$\n",
    "\n",
    "A popular approach for dealing with this problem is the Extended Kalman Filter (EKF).\n",
    "\n",
    "## 7.3 Additional remarks\n",
    "\n",
    "As in the previous chapters, we assume here that **data association** is given, that is, we know which feature is being seen by each sensor observation!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">OPTIONAL</span>\n",
    "\n",
    "<span style=\"color:green\">Surf the internet looking for more general information about SLAM. You can include additional definitions, examples, images, videos,... anything you find interesting!</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simultaneous Localization and Mapping (SLAM) is a computational framework used by robots and autonomous systems to construct a map of an unfamiliar environment while simultaneously pinpointing their location within it. This dual functionality is crucial for navigating and operating in environments where GPS signals are unavailable or unreliable, such as indoor spaces, underwater territories, or extraterrestrial surfaces.\n",
    "\n",
    "### **Core Elements of SLAM**\n",
    "\n",
    "1. **Sensors:** SLAM systems rely on a variety of sensors to gather information about their surroundings. Commonly used sensors include cameras (monocular, stereo, or RGB-D), LiDAR (Light Detection and Ranging), ultrasonic sensors, and inertial measurement units (IMUs). These devices capture data about the environment and the system’s movement.\n",
    "\n",
    "2. **Feature Detection:** The system identifies unique landmarks or features in the sensor data, such as edges, corners, or textures. These features act as points of reference for both mapping and localization.\n",
    "\n",
    "3. **Data Matching:** SLAM involves comparing new observations with previously detected features. This process, known as data association, helps refine the map and update the system's positional estimate.\n",
    "\n",
    "4. **State Estimation:** To determine the system’s position and orientation, SLAM employs algorithms like the Extended Kalman Filter (EKF) or Particle Filter. These techniques account for uncertainties and sensor inaccuracies to enhance precision.\n",
    "\n",
    "5. **Map Modeling:** The environment is depicted using different map formats, such as occupancy grids, feature-based maps, or dense 3D reconstructions, depending on the specific application.\n",
    "\n",
    "### **Applications of SLAM**\n",
    "\n",
    "- **Autonomous Driving:** SLAM plays a key role in enabling self-driving cars to navigate through complex environments by continuously mapping and localizing in real-time.\n",
    "\n",
    "- **Robotics:** SLAM is utilized by robots, drones, and autonomous vacuum cleaners for tasks such as cleaning, delivery, and inspection in changing environments.\n",
    "\n",
    "- **Augmented Reality (AR):** AR systems leverage SLAM to seamlessly overlay virtual objects onto the physical world, ensuring proper alignment and enhancing the user experience.\n",
    "\n",
    "### **Challenges in SLAM**\n",
    "\n",
    "- **Dynamic Settings:** Environments with moving objects or frequent changes can disrupt feature tracking and data matching, leading to errors in localization and mapping.\n",
    "\n",
    "- **High Computational Demand:** SLAM requires significant processing power to handle large volumes of sensor data and update maps in real time.\n",
    "\n",
    "- **Sensor Constraints:** Factors such as noise, limited sensor range, and environmental conditions (e.g., poor lighting) can reduce the reliability and accuracy of SLAM systems.\n",
    "\n",
    "### **Innovations in SLAM**\n",
    "\n",
    "Recent advancements incorporate machine learning to improve the efficiency of feature detection and data association, particularly in dynamic and complex settings. Additionally, combining data from multiple sensor types—such as integrating visual and LiDAR inputs—has resulted in more robust and precise SLAM solutions. \n",
    "\n",
    "This evolving technology continues to expand its capabilities, pushing the boundaries of autonomous navigation and interaction across various domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">***END OF OPTIONAL PART***</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
